{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/sgolkar/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub, os\n",
    "# import wandb\n",
    "token = \"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\"\n",
    "huggingface_hub.login(token=token, add_to_git_credential=True)\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"train_KCroberta_mlp.ipynb\"\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the tokenizer and the tokenized dataset with numbers extracted\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import DatasetDict\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"../toKCenizer_lorenz.json\",\n",
    "    bos_token=\"[END]\",\n",
    "    eos_token=\"[END]\",\n",
    "    mask_token=\"?\",\n",
    "    pad_token=\"[PAD]\",\n",
    ")\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "ds_path = \"/mnt/home/sgolkar/ceph/datasets/microcosm/lorenz_world_xsmall/clean/\"\n",
    "tokenized_ds = DatasetDict.load_from_disk(ds_path + \"toKCenized_xslorenz_ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the new collator type with numbers\n",
    "\n",
    "from KCroberta_mlp import KC_mlm_collator\n",
    "\n",
    "KC_coll = KC_mlm_collator(tokenizer=tokenizer, mlm_probability=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "720 76,702,380\n"
     ]
    }
   ],
   "source": [
    "# defining the roberta derived model\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "from KCroberta_mlp import KCRobertaForMaskedLMMLP\n",
    "\n",
    "hidden_size = 720\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=1150,\n",
    "    num_attention_heads=6,\n",
    "    num_hidden_layers=12,\n",
    "    type_vocab_size=2,\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=4 * hidden_size,\n",
    ")\n",
    "\n",
    "number_token = tokenizer(\"#\")['input_ids'][0]\n",
    "print(number_token)\n",
    "\n",
    "model = KCRobertaForMaskedLMMLP(config=config, power_num=1 / 3,\n",
    "        zero_others=True,\n",
    "        multiply_num_embedding=False,\n",
    "        number_token=number_token)\n",
    "\n",
    "print(hidden_size, f\"{model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /mnt/home/sgolkar/ceph/datasets/microcosm/lorenz_world_xsmall/clean/toKCenized_xslorenz_ds/train/cache-c9d887003ea829c7.arrow and /mnt/home/sgolkar/ceph/datasets/microcosm/lorenz_world_xsmall/clean/toKCenized_xslorenz_ds/train/cache-0599c8ac82cfb824.arrow\n"
     ]
    }
   ],
   "source": [
    "#  defining a small dataset for testing the model\n",
    "\n",
    "train_size = 800_000\n",
    "test_size = 5000\n",
    "\n",
    "downsampled_dataset = tokenized_ds[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dense2_numb = model.roberta.embeddings.dense2_numb.weight.data[0,0].item()\n",
    "old_dense_head = model.number_head.dense.weight.data[0,0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the trainer\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./KCroberta_xslorenz\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=5000,\n",
    "    eval_steps=5000,\n",
    "    logging_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    learning_rate=0.002,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.0001,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    return {\n",
    "        \"loss_mlm\": eval_preds[0][0].mean(),\n",
    "        \"loss_numbers\": eval_preds[0][1].mean(),\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=KC_coll,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/sgolkar/anaconda3/envs/gptLY/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgolkar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/sgolkar/projects/microcosm/KCroberta_mlp/wandb/run-20230610_123000-szbxnkg4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/golkar/huggingface/runs/szbxnkg4' target=\"_blank\">neat-sound-39</a></strong> to <a href='https://wandb.ai/golkar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/golkar/huggingface' target=\"_blank\">https://wandb.ai/golkar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/golkar/huggingface/runs/szbxnkg4' target=\"_blank\">https://wandb.ai/golkar/huggingface/runs/szbxnkg4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='500000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   252/500000 01:45 < 58:37:32, 2.37 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/gptLY/lib/python3.10/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1667\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/gptLY/lib/python3.10/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1927\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1928\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1929\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1931\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1932\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1933\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1934\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1935\u001b[0m ):\n\u001b[1;32m   1936\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/gptLY/lib/python3.10/site-packages/transformers/trainer.py:2717\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2715\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   2716\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2717\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   2719\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/anaconda3/envs/gptLY/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/gptLY/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.011966008692979813\n",
      "-0.002396566793322563\n"
     ]
    }
   ],
   "source": [
    "print(old_dense2_numb- model.roberta.embeddings.dense2_numb.weight.data[0,0].item())\n",
    "print(old_dense_head- model.number_head.dense.weight.data[0,0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "KC_coll = KC_mlm_collator(tokenizer=tokenizer, mlm_probability=0.2)\n",
    "masked_sample = KC_coll([tokenized_ds[\"train\"][5], \n",
    "                         tokenized_ds[\"train\"][6],\n",
    "                         tokenized_ds[\"train\"][7],\n",
    "                         tokenized_ds[\"train\"][8],\n",
    "                         tokenized_ds[\"train\"][9],\n",
    "                         tokenized_ds[\"train\"][10]])\n",
    "for key, val in masked_sample.items():\n",
    "    masked_sample[key] = masked_sample[key].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2908, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(masked_sample['input_ids']==number_token).unsqueeze(-1).type_as(masked_sample['masked_numbers']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1388, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(masked_sample['input_ids']==tokenizer.mask_token_id).unsqueeze(-1).type_as(masked_sample['masked_numbers']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.6390, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor(1.1216, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " tensor(0.5173, device='cuda:0', grad_fn=<MseLossBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "out = model(**masked_sample)\n",
    "out.loss, out.loss_mlm, out.loss_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.6100, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor(1.1152, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " tensor(0.4948, device='cuda:0', grad_fn=<MseLossBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "out = model(**masked_sample)\n",
    "out.loss, out.loss_mlm, out.loss_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted token:  #\n",
      "masked token: ?\n",
      "actual token: #\n",
      "\n",
      "predicted number: 0.64\n",
      "masked number: 1.00\n",
      "actual number: 12.13\n"
     ]
    }
   ],
   "source": [
    "import copy, torch\n",
    "\n",
    "\n",
    "num_token = tokenizer.encode('#')[0]\n",
    "mask_token = tokenizer.encode('?')[0]\n",
    "\n",
    "sample = tokenized_ds[\"val\"][1]\n",
    "\n",
    "masked_sample = copy.deepcopy(sample)\n",
    "masked_sample['input_ids'][10] = mask_token\n",
    "len_ = len(masked_sample['input_ids'])\n",
    "masked_sample['masked_numbers'] = copy.deepcopy(sample['numbers'])[:len_]\n",
    "masked_sample['masked_numbers'][10] = 1.0\n",
    "ans = masked_sample['numbers'][10]\n",
    "masked_sample['numbers'] = masked_sample['masked_numbers'][:len_]\n",
    "masked_sample['labels'] = sample['input_ids']\n",
    "\n",
    "for key, val in masked_sample.items():\n",
    "    masked_sample[key] = torch.tensor(val).reshape(1, -1).cuda()\n",
    "masked_sample['labels'] = 0*masked_sample['labels']-100\n",
    "masked_sample['labels'][0,10] = sample['input_ids'][10]\n",
    "\n",
    "out = model(**masked_sample)\n",
    "\n",
    "print('predicted token: ', tokenizer.decode([out.logits[0,10].argmax().item()]))\n",
    "print('masked token:', tokenizer.decode(masked_sample['input_ids'][0,10]))\n",
    "print('actual token:', tokenizer.decode(sample['input_ids'][10]))\n",
    "print()\n",
    "print('predicted number: {:.2f}'.format(out.numbers[0,10].item()))\n",
    "print('masked number: {:.2f}'.format(masked_sample['masked_numbers'][0,10].item()))\n",
    "print('actual number: {:.2f}'.format(sample['numbers'][10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the huggingface model via pickle \n",
    "\n",
    "torch.save(model, 'model_torch_save.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"model_pickle_save.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(model, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptLY",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
