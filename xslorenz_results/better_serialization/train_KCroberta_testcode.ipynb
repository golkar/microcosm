{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/sgolkar/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgolkar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import huggingface_hub, wandb, os\n",
    "\n",
    "token = \"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\"\n",
    "huggingface_hub.login(token=token, add_to_git_credential=True)\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"train_KCroberta_testcode.ipynb\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the tokenizer and the tokenized dataset with numbers extracted\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import DatasetDict\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"tokenizer.json\",\n",
    "    bos_token=\"[END]\",\n",
    "    eos_token=\"[END]\",\n",
    "    mask_token=\"?\",\n",
    "    pad_token=\"[PAD]\",\n",
    ")\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "ds_path = \"/mnt/home/sgolkar/ceph/datasets/microcosm/lorenz_world_xsmall/clean/\"\n",
    "tokenized_ds = DatasetDict.load_from_disk(ds_path + \"numenc_shortbrack_xslorenz_ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the new collator type with numbers\n",
    "\n",
    "from KCroberta import KC_mlm_collator\n",
    "\n",
    "KC_coll = KC_mlm_collator(tokenizer=tokenizer, mlm_probability=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50,466,597\n"
     ]
    }
   ],
   "source": [
    "# defining the roberta derived model\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "from KCroberta import KCRobertaForMaskedLM\n",
    "\n",
    "\n",
    "model_config = RobertaConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=597,\n",
    "    hidden_size=640,\n",
    "    num_hidden_layers=10,\n",
    "    num_attention_heads=10,\n",
    "    intermediate_size=4*640,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0,\n",
    "    attention_probs_dropout_prob=0,\n",
    ")\n",
    "model = KCRobertaForMaskedLM(config=model_config)\n",
    "\n",
    "print(f\"{model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /mnt/home/sgolkar/ceph/datasets/microcosm/lorenz_world_xsmall/clean/numenc_shortbrack_xslorenz_ds/train/cache-ab14fed9d321f235.arrow and /mnt/home/sgolkar/ceph/datasets/microcosm/lorenz_world_xsmall/clean/numenc_shortbrack_xslorenz_ds/train/cache-a70190da3e242a74.arrow\n"
     ]
    }
   ],
   "source": [
    "#  defining a small dataset for testing the model\n",
    "\n",
    "train_size = 100000\n",
    "test_size = 100\n",
    "\n",
    "downsampled_dataset = tokenized_ds[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the trainer\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# defining the training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=200,\n",
    "    report_to=\"wandb\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=5000,\n",
    "    eval_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=0.0001,\n",
    "    warmup_steps=5000,\n",
    "    weight_decay=0.0001,\n",
    "    fp16=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    ddp_find_unused_parameters=False,\n",
    ")\n",
    "\n",
    "# Reporting the mlm loss and the numbers loss\n",
    "def compute_metrics(eval_preds):\n",
    "    return {\n",
    "        \"loss_mlm\": eval_preds[0][0].mean(),\n",
    "        \"loss_numbers\": eval_preds[0][1].mean(),\n",
    "    }\n",
    "\n",
    "\n",
    "# defining the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=KC_coll,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/sgolkar/anaconda3/envs/gpt/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/sgolkar/projects/microcosm/xslorenz_results/better_serialization/wandb/run-20230614_210515-5cnskimy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/golkar/huggingface/runs/5cnskimy' target=\"_blank\">bright-pine-49</a></strong> to <a href='https://wandb.ai/golkar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/golkar/huggingface' target=\"_blank\">https://wandb.ai/golkar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/golkar/huggingface/runs/5cnskimy' target=\"_blank\">https://wandb.ai/golkar/huggingface/runs/5cnskimy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='584' max='15625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  584/15625 02:54 < 1:15:06, 3.34 it/s, Epoch 0.19/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.11/site-packages/transformers/trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1661\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1662\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1663\u001b[0m )\n\u001b[0;32m-> 1664\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1665\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1666\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1667\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1668\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1669\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.11/site-packages/transformers/trainer.py:1940\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1938\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1939\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1940\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1942\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1943\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1944\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1945\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1946\u001b[0m ):\n\u001b[1;32m   1947\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.11/site-packages/transformers/trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2734\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2735\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2737\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2738\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.11/site-packages/transformers/trainer.py:2767\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2766\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2767\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2768\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2769\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2770\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/microcosm/xslorenz_results/better_serialization/KCroberta.py:135\u001b[0m, in \u001b[0;36mKCRobertaForMaskedLM.forward\u001b[0;34m(self, input_ids, masked_numbers, numbers, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    125\u001b[0m     output \u001b[39m=\u001b[39m (prediction_scores,) \u001b[39m+\u001b[39m outputs[\u001b[39m2\u001b[39m:]\n\u001b[1;32m    126\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    127\u001b[0m         ((masked_lm_loss,) \u001b[39m+\u001b[39m output) \u001b[39mif\u001b[39;00m masked_lm_loss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m output\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m MaskedLMOutputWithNumbers(\n\u001b[1;32m    131\u001b[0m     loss\u001b[39m=\u001b[39mloss_total,\n\u001b[1;32m    132\u001b[0m     loss_mlm\u001b[39m=\u001b[39mmasked_lm_loss,\n\u001b[1;32m    133\u001b[0m     loss_numbers\u001b[39m=\u001b[39mmasked_numbers_loss,\n\u001b[1;32m    134\u001b[0m     logits\u001b[39m=\u001b[39mprediction_scores,\n\u001b[0;32m--> 135\u001b[0m     numbers\u001b[39m=\u001b[39mprediction_numbers\u001b[39m.\u001b[39msign() \u001b[39m*\u001b[39m (\u001b[39m10\u001b[39;49m \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m (prediction_numbers\u001b[39m.\u001b[39;49mabs() \u001b[39m-\u001b[39;49m \u001b[39m6\u001b[39;49m)),\n\u001b[1;32m    136\u001b[0m     hidden_states\u001b[39m=\u001b[39moutputs\u001b[39m.\u001b[39mhidden_states,\n\u001b[1;32m    137\u001b[0m     attentions\u001b[39m=\u001b[39moutputs\u001b[39m.\u001b[39mattentions,\n\u001b[1;32m    138\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.11/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[39mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.11/site-packages/torch/_tensor.py:882\u001b[0m, in \u001b[0;36mTensor.__rpow__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[39m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[1;32m    880\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__rpow__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m    881\u001b[0m     dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mresult_type(other, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 882\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtensor(other, dtype\u001b[39m=\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2078e-05, 2.4255e-06, 2.0575e-05,  ..., 5.6107e-06, 2.9893e-05,\n",
       "         1.2698e-05],\n",
       "        [2.1896e-05, 3.3559e-05, 2.0343e-05,  ..., 1.7681e-05, 2.0253e-05,\n",
       "         1.7347e-05],\n",
       "        [1.8781e-05, 2.8144e-05, 1.7481e-05,  ..., 1.6978e-05, 1.4513e-05,\n",
       "         1.4465e-05],\n",
       "        [6.6078e-06, 3.5232e-05, 2.1444e-05,  ..., 1.8532e-05, 2.1221e-05,\n",
       "         1.8182e-05]], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = KC_coll([downsampled_dataset[\"test\"][0], downsampled_dataset[\"test\"][1], downsampled_dataset[\"test\"][1], downsampled_dataset[\"test\"][1]])\n",
    "\n",
    "for key in input.keys():\n",
    "    input[key] = input[key].cuda()\n",
    "\n",
    "# checking for nan in the output\n",
    "output = model(**input)\n",
    "output.numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(929.6164, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.loss_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptLY",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
