#!/bin/bash
#SBATCH -p gpu --gpus=2 -C a100-80gb -n 24  -N 1 -t 44:0:0

export OMP_NUM_THREADS=4
export TRANSFORMERS_CACHE=/mnt/home/sgolkar/projects/microcosm/cache

source ~/anaconda3/etc/profile.d/conda.sh
master_node=$SLURMD_NODENAME
conda activate gptLY
cd ~/projects/microcosm
echo job_id $SLURM_JOB_ID
echo num_nodes $SLURM_JOB_NUM_NODES
echo gpus_per_node $SLURM_GPUS_PER_NODE
echo world_size $WORLD_SIZE
torchrun --standalone \
        --nnodes 1 \
        --nproc_per_node 2 \
        --rdzv_id $SLURM_JOB_ID \
        --rdzv_backend c10d \
        --rdzv_endpoint $master_node:$SLURM_JOB_ID \
        mroberta_xslorenz_resume.py --run_id rw7ta38d --chkpt_path /mnt/home/sgolkar/ceph/saves/xslorenz/mroberta/wandb/run-20230531_162250-rw7ta38d/files/model/checkpoint-40000