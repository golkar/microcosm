{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/sgolkar/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgolkar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import huggingface_hub, wandb, os\n",
    "\n",
    "token = \"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\"\n",
    "huggingface_hub.login(token=token, add_to_git_credential=True)\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"train_KCroberta.py\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the tokenizer and the tokenized dataset with numbers extracted\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import DatasetDict\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"toKCenizer_lorenz.json\",\n",
    "    bos_token=\"[END]\",\n",
    "    eos_token=\"[END]\",\n",
    "    mask_token=\"?\",\n",
    "    pad_token=\"[PAD]\",\n",
    ")\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "ds_path = \"/mnt/home/sgolkar/ceph/datasets/microcosm/lorenz_world_xsmall/clean/\"\n",
    "tokenized_ds = DatasetDict.load_from_disk(ds_path + \"toKCenized_xslorenz_ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the new collator type with numbers\n",
    "\n",
    "from KCroberta import KC_mlm_collator\n",
    "\n",
    "KC_coll = KC_mlm_collator(tokenizer=tokenizer, mlm_probability=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720 76,654,108\n"
     ]
    }
   ],
   "source": [
    "# defining the roberta derived model\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "from KCroberta import KCRobertaForMaskedLM\n",
    "\n",
    "hidden_size = 720\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=1150,\n",
    "    num_attention_heads=6,\n",
    "    num_hidden_layers=12,\n",
    "    type_vocab_size=2,\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=4 * hidden_size,\n",
    ")\n",
    "\n",
    "model = KCRobertaForMaskedLM(config=config, power_num=1 / 3)\n",
    "\n",
    "print(hidden_size, f\"{model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /mnt/home/sgolkar/ceph/datasets/microcosm/lorenz_world_xsmall/clean/toKCenized_xslorenz_ds/train/cache-c9d887003ea829c7.arrow and /mnt/home/sgolkar/ceph/datasets/microcosm/lorenz_world_xsmall/clean/toKCenized_xslorenz_ds/train/cache-0599c8ac82cfb824.arrow\n"
     ]
    }
   ],
   "source": [
    "#  defining a small dataset for testing the model\n",
    "\n",
    "train_size = 800_000\n",
    "test_size = 5000\n",
    "\n",
    "downsampled_dataset = tokenized_ds[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the trainer\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./KCroberta_xslorenz\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=5000,\n",
    "    eval_steps=5000,\n",
    "    logging_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    learning_rate=0.00002,\n",
    "    warmup_steps=2000,\n",
    "    weight_decay=0.0001,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    return {\n",
    "        \"loss_mlm\": eval_preds[0][0].mean(),\n",
    "        \"loss_numbers\": eval_preds[0][1].mean(),\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=KC_coll,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/sgolkar/anaconda3/envs/gptLY/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/sgolkar/projects/microcosm/wandb/run-20230606_211123-e0jiobwg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/golkar/huggingface/runs/e0jiobwg' target=\"_blank\">ruby-sun-30</a></strong> to <a href='https://wandb.ai/golkar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/golkar/huggingface' target=\"_blank\">https://wandb.ai/golkar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/golkar/huggingface/runs/e0jiobwg' target=\"_blank\">https://wandb.ai/golkar/huggingface/runs/e0jiobwg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='915' max='500000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   915/500000 06:28 < 58:59:01, 2.35 it/s, Epoch 0.01/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.6320, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor(1.1180, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " tensor(0.5140, device='cuda:0', grad_fn=<MseLossBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KC_coll = KC_mlm_collator(tokenizer=tokenizer, mlm_probability=0.2)\n",
    "masked_sample = KC_coll([tokenized_ds[\"train\"][3], tokenized_ds[\"train\"][4]])\n",
    "for key, val in masked_sample.items():\n",
    "    masked_sample[key] = masked_sample[key].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.6390, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor(1.1216, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " tensor(0.5173, device='cuda:0', grad_fn=<MseLossBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "out = model(**masked_sample)\n",
    "out.loss, out.loss_mlm, out.loss_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.6100, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor(1.1152, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " tensor(0.4948, device='cuda:0', grad_fn=<MseLossBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "out = model(**masked_sample)\n",
    "out.loss, out.loss_mlm, out.loss_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted token:  #\n",
      "masked token: ?\n",
      "actual token: #\n",
      "\n",
      "predicted number: 0.64\n",
      "masked number: 1.00\n",
      "actual number: 12.13\n"
     ]
    }
   ],
   "source": [
    "import copy, torch\n",
    "\n",
    "\n",
    "num_token = tokenizer.encode('#')[0]\n",
    "mask_token = tokenizer.encode('?')[0]\n",
    "\n",
    "sample = tokenized_ds[\"val\"][1]\n",
    "\n",
    "masked_sample = copy.deepcopy(sample)\n",
    "masked_sample['input_ids'][10] = mask_token\n",
    "len_ = len(masked_sample['input_ids'])\n",
    "masked_sample['masked_numbers'] = copy.deepcopy(sample['numbers'])[:len_]\n",
    "masked_sample['masked_numbers'][10] = 1.0\n",
    "ans = masked_sample['numbers'][10]\n",
    "masked_sample['numbers'] = masked_sample['masked_numbers'][:len_]\n",
    "masked_sample['labels'] = sample['input_ids']\n",
    "\n",
    "for key, val in masked_sample.items():\n",
    "    masked_sample[key] = torch.tensor(val).reshape(1, -1).cuda()\n",
    "masked_sample['labels'] = 0*masked_sample['labels']-100\n",
    "masked_sample['labels'][0,10] = sample['input_ids'][10]\n",
    "\n",
    "out = model(**masked_sample)\n",
    "\n",
    "print('predicted token: ', tokenizer.decode([out.logits[0,10].argmax().item()]))\n",
    "print('masked token:', tokenizer.decode(masked_sample['input_ids'][0,10]))\n",
    "print('actual token:', tokenizer.decode(sample['input_ids'][10]))\n",
    "print()\n",
    "print('predicted number: {:.2f}'.format(out.numbers[0,10].item()))\n",
    "print('masked number: {:.2f}'.format(masked_sample['masked_numbers'][0,10].item()))\n",
    "print('actual number: {:.2f}'.format(sample['numbers'][10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the huggingface model via pickle \n",
    "\n",
    "torch.save(model, 'model_torch_save.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"model_pickle_save.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(model, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptLY",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
